delta_q = delta_q - 2*(y[i]-u[i]/v[i])*Di
}
s = s + delta_q^2
lam = lam - eta*delta_q/sqrt(s+epsilon)
lam.df = rbind(lam.df, lam)
count = count + 1
}
return(list(lam.df, cost_vec))
}
train_result = learn_para(X,y)
train_result
lam.df = train_result[[1]]
lam = lam.df[29,]
# caculate matrix L that contain L_i
L = sapply(1:n, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y)/length(y))
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
train_result
knitr::opts_chunk$set(echo = TRUE)
data = read.table("balance-scale.txt",header = FALSE, sep = ',')
num.row = dim(data)[1]
train_index = sample(num.row,floor(num.row*.7))
train = data[train_index,]; test = data[-train_index,]
y = train[,1]; y = as.numeric(y)
X = train[,-1]
y_test = test[,1]; y_test = as.numeric(y_test)
X_test = test[,-1]
remove(data,train)
dimension = dim(X); n = dimension[1]
r = dimension[2]
# DEBUGGING
eta= 0.001; min_samples = 1000; precision = 0.001
cov(X)
X[1:3,]
hist(X)
hist(X[,1])
hist(X[,2])
hist(X[,3])
hist(X[,4])
hist(X[,5])
plot(X[,1])
plot(X[,2])
plot(X[,3])
plot(X[,4])
# learn parameter without splitting the train set
learn_para <- function(X,y,eta= 0.1, max_iter = 30,precision = 0.001){
count= 0; old_cost = 0; epsilon = 0.0001
lam = rep(.9,r)
cost = 1; cost_vec = c()
lam.df = c()
while ((abs(cost-old_cost) > precision)&(count <= max_iter)){
old_cost = cost
s = rep(0,r)
# caculate matrix L that contain L_i
L = sapply(1:n, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
cost = sum((y - u/v)^2)   ; cost_vec = c(cost_vec,cost)
# sum((round(u/v)==y)/length(y))
delta_q = 0
for (i in 1:n){
Li_prime = sapply(1:r, function(k){
return((X[,k]!=X[i,k])*L[i,]/lam[k])
})
Li_prime = t(Li_prime)
ui_prime = sapply(1:r, function(l){return(y %*% Li_prime[l,])})
vi_prime = apply(Li_prime,1, sum)
Di = (v[i]*ui_prime - u[i]*vi_prime)/v[i]^2
delta_q = delta_q - 2*(y[i]-u[i]/v[i])*Di
}
s = s + delta_q^2
lam = lam - eta*delta_q/sqrt(s+epsilon)
lam.df = rbind(lam.df, lam)
count = count + 1
}
return(list(lam.df, cost_vec))
}
train_result = learn_para(X,y)
train_result
lam.df = train_result[[1]]
lam = lam.df[nrow(lam.df),]
L = sapply(1:n, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y)/length(y))
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
10^5
lam = lam.df[nrow(lam.df)-1,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-2,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-3,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-5,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-6,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-7,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-8,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
lam = lam.df[nrow(lam.df)-9,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
sum((round(u/v)==y_test)/n.test)
train_result
test_loss = c()
for (i in 1:30){
lam = lam.df[i,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
test_loss = c(test_loss,sum((round(u/v)==y_test)/n.test))
}
test_loss
max(test_loss)
?ksmooth
with(cars, {
plot(speed, dist)
lines(ksmooth(speed, dist, "normal", bandwidth = 2), col = 2)
lines(ksmooth(speed, dist, "normal", bandwidth = 5), col = 3)
})
with(cars, {
plot(speed, dist)
lines(ksmooth(speed, dist, "normal", bandwidth = 5), col = 3)
})
install.packages("np")
library("np")
data("cps71")
model.np <- npreg(logwage ~ age, regtype = "ll", bwmethod = "cv.aic",
gradients = TRUE, data = cps71)
summary(model.np)
cps.eval <- data.frame(age = seq(10,70, by=10))
predict(model.par, newdata = cps.eval)
predict(model.np, newdata = cps.eval)
?data("cps71")
?sample
# train, test split
train_index = sample(nrow(cps71),round(nrow(cps71)*.7))
train = cps71[train_index,]
test = cps71[-train_index,]
train
# train, test split
train_index = sample(nrow(cps71),round(nrow(cps71)*.7))
train = cps71[train_index,]
test = cps71[-train_index,]
model.np <- npreg(logwage ~ age, regtype = "ll", bwmethod = "cv.aic",
gradients = TRUE, data = train)
summary(model.np)
cps.eval <- data.frame(age = test$age)
predict(model.np, newdata = cps.eval)
test$logwage
result = predict(model.np, newdata = cps.eval)
sum((result-test$logwage)^2)
R> data("wage1")
data("wage1")
data("wage1")
bw.all <- npregbw(lwage ~ female + married + educ + exper + tenure,
regtype = "ll", bwmethod = "cv.aic", data = wage1)
model.np <- npreg(bws = bw.all)
summary(model.np)
train_index = sample(nrow(cps71),round(nrow(cps71)*.7))
train_index = sample(nrow(wage1),round(nrow(wage1)*.7))
train = wage1[train_index,]
test = wage1[-train_index,]
test_groundtruth_result = test$lwage
test_input = test
test_input$lwage <- NULL
head(test_input)
bw.all <- npregbw(lwage ~ female + married + educ + exper + tenure,
regtype = "ll", bwmethod = "cv.aic", data = train)
model.np <- npreg(bws = bw.all)
summary(model.np)
result = predict(model.np, newdata = test_input)
sum((result-test_groundtruth_result)^2)
npregbw()
npregbw
?npregbw
data = read.table("balance-scale.txt",header = FALSE, sep = ',')
num.row = dim(data)[1]
train_index = sample(num.row,floor(num.row*.7))
train = data[train_index,]; test = data[-train_index,]
y = train[,1]; y = as.numeric(y)
X = train[,-1]
y_test = test[,1]; y_test = as.numeric(y_test)
X_test = test[,-1]
data$V1
?npregbw
bw.all <- npregbw(V1 ~ V2 + V3 + V4, regtype = "ll", bwmethod = "cv.aic", data = train)
model.np <- npreg(bws = bw.all)
summary(model.np)
result = predict(model.np, newdata = X_test)
sum((result-y_test)^2)
y_test
sum(result==y_test)/length(y_test)
knitr::opts_chunk$set(echo = TRUE)
data = read.table("balance-scale.txt",header = FALSE, sep = ',')
num.row = dim(data)[1]
train_index = sample(num.row,floor(num.row*.7))
train = data[train_index,]; test = data[-train_index,]
y = train[,1]; y = as.numeric(y)
X = train[,-1]
y_test = test[,1]; y_test = as.numeric(y_test)
X_test = test[,-1]
remove(data,train)
dimension = dim(X); n = dimension[1]
r = dimension[2]
# DEBUGGING
eta= 0.001;  max_iter = 50; precision = 0.001
# DEBUGGING
eta= 0.01;  max_iter = 50; precision = 0.001
# learn parameter without splitting the train set
learn_para <- function(X,y,eta= 0.01, max_iter = 50,precision = 0.1){
count= 0; old_cost = 0; epsilon = 0.0001
lam = rep(.9,r)
cost = 1; cost_vec = c()
lam.df = c()
while ((abs(cost-old_cost) > precision)&(count <= max_iter)){
old_cost = cost
s = rep(0,r)
# caculate matrix L that contain L_i
L = sapply(1:n, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
cost = sum((y - u/v)^2)   ; cost_vec = c(cost_vec,cost)
# sum((round(u/v)==y)/length(y))
delta_q = 0
for (i in 1:n){
Li_prime = sapply(1:r, function(k){
return((X[,k]!=X[i,k])*L[i,]/lam[k])
})
Li_prime = t(Li_prime)
ui_prime = sapply(1:r, function(l){return(y %*% Li_prime[l,])})
vi_prime = apply(Li_prime,1, sum)
Di = (v[i]*ui_prime - u[i]*vi_prime)/v[i]^2
delta_q = delta_q - 2*(y[i]-u[i]/v[i])*Di
}
s = s + delta_q^2
lam = lam - eta*delta_q/sqrt(s+epsilon)
lam.df = rbind(lam.df, lam)
count = count + 1
}
return(list(lam.df, cost_vec))
}
train_result = learn_para(X,y)
train_result
lam.df = train_result[[1]]
# learn parameter without splitting the train set
learn_para <- function(X,y,eta= 0.01, max_iter = 50,precision = 0.1){
count= 0; old_cost = 0; epsilon = 0.0001
lam = rep(.39,r)
cost = 1; cost_vec = c()
lam.df = c()
while ((abs(cost-old_cost) > precision)&(count <= max_iter)){
old_cost = cost
s = rep(0,r)
# caculate matrix L that contain L_i
L = sapply(1:n, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
cost = sum((y - u/v)^2)   ; cost_vec = c(cost_vec,cost)
# sum((round(u/v)==y)/length(y))
delta_q = 0
for (i in 1:n){
Li_prime = sapply(1:r, function(k){
return((X[,k]!=X[i,k])*L[i,]/lam[k])
})
Li_prime = t(Li_prime)
ui_prime = sapply(1:r, function(l){return(y %*% Li_prime[l,])})
vi_prime = apply(Li_prime,1, sum)
Di = (v[i]*ui_prime - u[i]*vi_prime)/v[i]^2
delta_q = delta_q - 2*(y[i]-u[i]/v[i])*Di
}
s = s + delta_q^2
lam = lam - eta*delta_q/sqrt(s+epsilon)
lam.df = rbind(lam.df, lam)
count = count + 1
}
return(list(lam.df, cost_vec))
}
train_result = learn_para(X,y)
train_result
lam.df = train_result[[1]]
lam = lam.df[nrow(lam.df),]
lam = lam.df[nrow(lam.df)-1,]
n.test = dim(X_test)[1]
L = sapply(1:n.test, function(i){return(sapply(1:n,function(j){
return(prod(lam^(X[j,] != X[i,])))
}))})
L = t(L)
u = sapply(1:n.test, function(l){return(y %*% L[l,])})
v = apply(L, 1, sum)
test_loss = c(test_loss,sum((round(u/v)==y_test)/n.test))
sum((round(u/v)==y_test)/n.test)
setwd("~/pp/lda")
data("iris")
head(iris)
?lda
library(MASS)
?lda
library(MASS)
data("iris")
head(iris)
r <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
r$prior
library(MASS)
data("iris")
head(iris)
re <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
re$counts
library(MASS)
data("iris")
head(iris)
re <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
re$counts
re$means
re$scaling
re$svd
re$lev
library(MASS)
data("iris")
head(iris)
# prior là prior probability mà ta chọn cho  mỗi nhóm.
# thông thường, ta chọn  prior probability của các nhóm là bằng nhau
re <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
re$counts
re$lev
#  group means.
re$means
# ma trận chuyển các quan sát thành các discriminant functions,  và normalized
re$scaling
# singular values
re$svd
re$class
# using train-test split
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = sample(1:nrow(iris), nrow(iris)*.75))
# predict
pre = predict(object = r, newdata = iris[-train, ])
# using train-test split
train_index = sample(1:nrow(iris), nrow(iris)*.75)
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = train_index)
# predict
pre = predict(object = r, newdata = iris[-train_index, ])
library(MASS)
data("iris")
head(iris)
# prior là prior probability mà ta chọn cho  mỗi nhóm.
# thông thường, ta chọn  prior probability của các nhóm là bằng nhau
re <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
re$counts
re$lev
#  group means.
re$means
# ma trận chuyển các quan sát thành các discriminant functions,  và normalized
re$scaling
# singular values
re$svd
# using train-test split
train_index = sample(1:nrow(iris), nrow(iris)*.75)
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = train_index)
# predict
pre = predict(object = r, newdata = iris[-train_index, ])
# classification result:
pre$class
# using train-test split
train_index = sample(1:nrow(iris), nrow(iris)*.75)
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = train_index)
# predict
test = iris[-train_index, ]
pre = predict(object = r, newdata = test)
# classification result:
pre$class
# using train-test split
train_index = sample(1:nrow(iris), nrow(iris)*.75)
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = train_index)
# predict
test = iris[-train_index, ]
pre = predict(object = r, newdata = test)
# classification result:
pre$class
sum(pre$class == test$Species)/nrow(test)
library(MASS)
data("iris")
head(iris)
# prior là prior probability mà ta chọn cho  mỗi nhóm.
# thông thường, ta chọn  prior probability của các nhóm là bằng nhau
re <- lda(formula = Species ~ ., data = iris, prior = c(1,1,1)/3)
re$counts
re$lev
#  group means.
re$means
# ma trận chuyển các quan sát thành các discriminant functions,  và normalized
re$scaling
# singular values
re$svd
# using train-test split
train_index = sample(1:nrow(iris), nrow(iris)*.75)
r3 <- lda(Species ~ ., # training model
iris,
prior = c(1,1,1)/3,
subset = train_index)
# predict
test = iris[-train_index, ]
pre = predict(object = r, newdata = test)
# classification result:
pre$class
# percentage of correct prediction
sum(pre$class == test$Species)/nrow(test)
